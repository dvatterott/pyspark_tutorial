<section>
  <h2>Startup Docker and navigate to your jupyter notebook!</h2>
</section>
<section>
  <h2>Create Spark Session.</h2>
  <p>
    <a href="https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html#pyspark.sql.SparkSession">
      SparkSession Docs
    </a>
  <br><br>
  This is the container that will read in dataframes, register tables, execute commands on tables, cache tables, etc. 
  <br><br>
  I think of it as my spark kernel (like you have a python kernel when you enter 'python' into the terminal)
  </p>
</section>
<section>
	<h2>Navigate to <a href="http://localhost:4040/jobs/">http://localhost:4040/jobs/</a> in your browser.</h2>
</section>
<section>
  <h2>Spend lots of time here!!</h2>
  <p><a href="https://spark.apache.org/docs/2.3.0/monitoring.html#web-interfaces">Monitoring Docs</a>
  <br><br>
  This is the first place I look when optimizing a job.
  <br><br>
  We can see jobs, stages (occur within jobs), cached tables, individual executors, and lineage of dataframes.
  </p>
</section>
<section>
  <h2>Read data into spark.</h2>
  <p>
    <a href="https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader">
      DataFrameReader Docs
    </a>
  <br><br>
  Instructions for how Spark should read the data. 
  <br><br>
  Remember that all your executors will receive a (random?) portion of the data.
  </p>
</section>
<section>
  <h2>Quick note on data structures.</h2>
  <p>
    <a href="https://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#resilient-distributed-datasets-rdds">
      RDDs
    </a>
    <br><br>
    <a href="https://spark.apache.org/docs/2.3.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame">
      DataFrames
    </a>
  </p>
</section>
<section>
  <h2>So far so good??</h2>
  <iframe src="https://giphy.com/embed/ioeQEPFDeS8s8" width="480" height="292" frameBorder="0"></iframe>
</section>
